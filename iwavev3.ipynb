{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==================== INSTALLATIONS & IMPORTS ====================\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport PIL.Image as Image\nimport io\nimport requests\nfrom torchvision import transforms\nfrom torchvision.models import vgg19\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\nimport time\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nimport cv2\nfrom PIL import Image as PILImage\nfrom io import BytesIO\nimport warnings\nimport glob\nwarnings.filterwarnings('ignore')\n\nprint(\"âœ… All imports successful!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== OPTIMIZED CONFIGURATION ====================\n\nclass Config:\n    # Model parameters - optimized for compression\n    levels = 3\n    hidden_channels = 96  # Reduced for efficiency\n    num_mixtures = 5      # Increased for better entropy modeling\n    \n    # Training parameters\n    batch_size = 4\n    learning_rate = 1e-4\n    num_epochs = 50\n    \n    # Image parameters\n    image_size = 256\n    \n    # Compression optimization\n    rate_weight = 5e-4    # Increased for better compression\n    subband_importance = [1.0, 1.3, 1.3, 1.8]  # LL, LH, HL, HH\n    \n    # Paths\n    model_dir = \"/kaggle/working/iwavev3_models/\"\n    sample_images_dir = \"/kaggle/working/sample_images/\"\n    train_data_path = \"/kaggle/input/kodim-shivam/*\"\n\nconfig = Config()\n\n# Create directories\nos.makedirs(config.model_dir, exist_ok=True)\nos.makedirs(config.sample_images_dir, exist_ok=True)\n\nprint(\"âœ… Configuration setup complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== OPTIMIZED DATASET CLASS ====================\n\nclass ImageCompressionDataset(Dataset):\n    def __init__(self, image_paths, image_size=256):\n        self.image_paths = image_paths\n        self.image_size = image_size\n        self.transform = transforms.Compose([\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor()\n        ])\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        try:\n            image = PILImage.open(image_path).convert('RGB')\n            image_tensor = self.transform(image)\n            return image_tensor, image_path\n        except Exception as e:\n            print(f\" Error loading {image_path}: {e}\")\n            return torch.zeros(3, self.image_size, self.image_size), image_path\ndef custom_collate_fn(batch):\n    images, paths = zip(*batch)\n    images = torch.stack(images, 0)\n    return images, paths\n\nprint(\" Dataset class defined!\")\n\n# ==================== OPTIMIZED MODEL COMPONENTS ====================\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels=96):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        \n    def forward(self, x):\n        residual = x\n        x = self.relu(self.conv1(x))\n        x = self.conv2(x)\n        return x + residual","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#TRANSFORMS\nclass AdditiveTransformUnit(nn.Module):\n    def __init__(self, in_channels=3, hidden_channels=96):\n        super(AdditiveTransformUnit, self).__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels, hidden_channels, 5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_channels, hidden_channels, 5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_channels, in_channels, 5, padding=2)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\nclass AffineTransformUnit(nn.Module):\n    def __init__(self, in_channels=3, hidden_channels=96):\n        super(AffineTransformUnit, self).__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels, hidden_channels, 5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_channels, hidden_channels, 5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_channels, in_channels * 2, 5, padding=2)\n        )\n    \n    def forward(self, x):\n        output = self.net(x)\n        shift, scale = output.chunk(2, dim=1)\n        scale = torch.sigmoid(scale) * 1.5 + 0.5\n        return shift, scale","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class WaveletLikeTransform(nn.Module):\n    def __init__(self, levels=3, transform_type='affine'):\n        super(WaveletLikeTransform, self).__init__()\n        self.levels = levels\n        self.transform_type = transform_type\n        \n        if transform_type == 'additive':\n            self.P_units = nn.ModuleList([AdditiveTransformUnit(3, config.hidden_channels) for _ in range(levels)])\n            self.U_units = nn.ModuleList([AdditiveTransformUnit(3, config.hidden_channels) for _ in range(levels)])\n        elif transform_type == 'affine':\n            self.P_units = nn.ModuleList([AffineTransformUnit(3, config.hidden_channels) for _ in range(levels)])\n            self.U_units = nn.ModuleList([AffineTransformUnit(3, config.hidden_channels) for _ in range(levels)])\n    \n    def ensure_divisible(self, x, divisor=32):\n        h, w = x.shape[2], x.shape[3]\n        new_h = ((h + divisor - 1) // divisor) * divisor\n        new_w = ((w + divisor - 1) // divisor) * divisor\n        \n        if new_h != h or new_w != w:\n            x = F.interpolate(x, size=(new_h, new_w), mode='bilinear', align_corners=False)\n        return x\n    \n    def forward_single_level(self, x, level):\n        batch, channels, height, width = x.shape\n        \n        if height % 2 != 0:\n            x = F.pad(x, (0, 0, 0, 1), mode='reflect')\n        if width % 2 != 0:\n            x = F.pad(x, (0, 1, 0, 0), mode='reflect')\n        height, width = x.shape[2], x.shape[3]\n        \n        x_ll = x[:, :, 0::2, 0::2]\n        x_lh = x[:, :, 0::2, 1::2]\n        x_hl = x[:, :, 1::2, 0::2]\n        x_hh = x[:, :, 1::2, 1::2]\n        \n        if self.transform_type == 'additive':\n            h_temp = x_hh - self.P_units[level](x_ll)\n            l_temp = x_ll + self.U_units[level](h_temp)\n            \n            hl_temp = x_hl - self.P_units[level](l_temp)\n            lh_temp = x_lh - self.P_units[level](l_temp)\n            \n            ll = l_temp + self.U_units[level](hl_temp) + self.U_units[level](lh_temp)\n            lh = lh_temp\n            hl = hl_temp\n            hh = h_temp\n        else:\n            shift_p, scale_p = self.P_units[level](x_ll)\n            h_temp = scale_p * (x_hh - shift_p)\n            \n            shift_u, scale_u = self.U_units[level](h_temp)\n            l_temp = scale_u * (x_ll + shift_u)\n            \n            shift_p2, scale_p2 = self.P_units[level](l_temp)\n            hl_temp = scale_p2 * (x_hl - shift_p2)\n            lh_temp = scale_p2 * (x_lh - shift_p2)\n            \n            shift_u2, scale_u2 = self.U_units[level](hl_temp)\n            ll = scale_u2 * (l_temp + shift_u2)\n            \n            lh = lh_temp\n            hl = hl_temp\n            hh = h_temp\n        \n        return ll, lh, hl, hh\n    def forward(self, x):\n        divisor = 2 ** self.levels\n        x = self.ensure_divisible(x, divisor)\n        \n        subbands = []\n        current = x\n        \n        for level in range(self.levels):\n            ll, lh, hl, hh = self.forward_single_level(current, level)\n            subbands.extend([lh, hl, hh])\n            current = ll\n        \n        subbands.append(current)\n        return subbands\n    \n    def inverse_single_level(self, ll, lh, hl, hh, level):\n        if self.transform_type == 'additive':\n            l_temp = ll - self.U_units[level](hl) - self.U_units[level](lh)\n            x_lh = lh + self.P_units[level](l_temp)\n            x_hl = hl + self.P_units[level](l_temp)\n            \n            x_ll = l_temp - self.U_units[level](hh)\n            x_hh = hh + self.P_units[level](x_ll)\n         else:\n            shift_u2, scale_u2 = self.U_units[level](hl)\n            l_temp = (ll / scale_u2) - shift_u2\n            \n            shift_p2, scale_p2 = self.P_units[level](l_temp)\n            x_lh = (lh / scale_p2) + shift_p2\n            x_hl = (hl / scale_p2) + shift_p2\n            \n            shift_u, scale_u = self.U_units[level](hh)\n            x_ll = (l_temp / scale_u) - shift_u\n            \n            shift_p, scale_p = self.P_units[level](x_ll)\n            x_hh = (hh / scale_p) + shift_p\n        \n        batch, channels, height, width = ll.shape\n        reconstructed = torch.zeros(batch, channels, height * 2, width * 2, device=ll.device)\n        \n        reconstructed[:, :, 0::2, 0::2] = x_ll\n        reconstructed[:, :, 0::2, 1::2] = x_lh\n        reconstructed[:, :, 1::2, 0::2] = x_hl\n        reconstructed[:, :, 1::2, 1::2] = x_hh\n        \n        return reconstructed\n    \n                \n    def inverse(self, subbands):\n        current = subbands[-1]\n        \n        for level in reversed(range(self.levels)):\n            lh = subbands[level * 3]\n            hl = subbands[level * 3 + 1]\n            hh = subbands[level * 3 + 2]\n            current = self.inverse_single_level(current, lh, hl, hh, level)\n        \n        return current","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#IMPROVED ENTROPY MODEL CLASS\nclass ImprovedEntropyModel(nn.Module):\n    def __init__(self, num_mixtures=5):\n        super(ImprovedEntropyModel, self).__init__()\n        self.num_mixtures = num_mixtures\n        self.context_net = nn.Sequential(\n            nn.Conv2d(3, 128, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, num_mixtures * 3, 3, padding=1)\n        )\n    \n    def forward(self, subbands):\n        entropy_params = []\n        for subband in subbands:\n            params = self.context_net(subband)\n            weights = F.softmax(params[:, :self.num_mixtures], dim=1)\n            means = torch.tanh(params[:, self.num_mixtures:2*self.num_mixtures]) * 1.0  # Tighter range\n            scales = torch.exp(torch.clamp(params[:, 2*self.num_mixtures:3*self.num_mixtures], -3, 3))  # Tighter scales\n            entropy_params.append((weights, means, scales))\n        return entropy_params","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#DequantizationModule\nclass DequantizationModule(nn.Module):\n    def __init__(self, channels=96):\n        super(DequantizationModule, self).__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(3, channels, 3, padding=1),\n            nn.ReLU(),\n            ResidualBlock(channels),\n            ResidualBlock(channels),\n            nn.Conv2d(channels, 3, 3, padding=1),\n            nn.Tanh()\n        )\n        \n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#PERCEPTUAL POST PROCESSING\nclass PerceptualPostProcessing(nn.Module):\n    def __init__(self, channels=96):\n        super(PerceptualPostProcessing, self).__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(3, channels, 3, padding=1),\n            nn.ReLU(),\n            ResidualBlock(channels),\n            nn.Conv2d(channels, 3, 3, padding=1),\n            nn.Tanh()\n        )\n    \n    def forward(self, x):\n        return self.net(x) + x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class iWaveV3_Base(nn.Module):\n    def __init__(self, levels=3, transform_type='affine'):\n        super(iWaveV3_Base, self).__init__()\n        self.levels = levels\n        self.transform_type = transform_type\n        \n        self.transform = WaveletLikeTransform(levels, transform_type)\n        self.entropy_model = ImprovedEntropyModel(config.num_mixtures)\n        self.qstep = nn.Parameter(torch.tensor(0.03))  # Slightly higher for compression\n    \n    def adaptive_quantize(self, x, subband_idx):\n        \"\"\"Adaptive quantization based on subband importance\"\"\"\n        importance = config.subband_importance[subband_idx % len(config.subband_importance)]\n        effective_qstep = self.qstep * importance\n        \n        if self.training:\n            noise = (torch.rand_like(x) - 0.5) * effective_qstep\n            return x + noise\n        else:\n            return torch.round(x / effective_qstep) * effective_qstep\n            \n    def channel_aware_quantize(self, x, subband_idx):\n        \"\"\"Different quantization for luminance vs chrominance\"\"\"\n        if self.training:\n            # Convert to YCbCr for better compression\n            if x.shape[1] == 3:  # RGB image\n                ycbcr = rgb_to_ycbcr(x)\n                # Luminance (Y) gets finer quantization\n                qsteps = torch.tensor([0.02, 0.04, 0.04]).to(x.device)\n                noise = (torch.rand_like(ycbcr) - 0.5) * qsteps.view(1, 3, 1, 1)\n                quantized = ycbcr + noise\n                return ycbcr_to_rgb(quantized)\n            else:\n                return self.adaptive_quantize(x, subband_idx)\n        else:\n            if x.shape[1] == 3:\n                ycbcr = rgb_to_ycbcr(x)\n                qsteps = torch.tensor([0.02, 0.04, 0.04]).to(x.device)\n                quantized = torch.round(ycbcr / qsteps.view(1, 3, 1, 1)) * qsteps.view(1, 3, 1, 1)\n                return ycbcr_to_rgb(quantized)\n            else:\n                return self.adaptive_quantize(x, subband_idx)\n                \n    def calculate_rate_improved(self, quantized_subbands, entropy_params):\n        \"\"\"Improved rate calculation with better probability modeling\"\"\"\n        total_rate = 0\n        for i, (subband, (weights, means, scales)) in enumerate(zip(quantized_subbands, entropy_params)):\n            subband_prob = 0\n            for k in range(config.num_mixtures):\n                # Use Laplace distribution for heavier tails (better for compression)\n                scale = torch.clamp(scales[:, k:k+1], min=1e-6)\n                log_prob = -torch.abs(subband - means[:, k:k+1]) / scale - torch.log(2 * scale)\n                prob_component = weights[:, k:k+1] * torch.exp(log_prob)\n                subband_prob += prob_component\n            \n            # Add small epsilon and use log1p for numerical stability\n            rate = -torch.log(torch.clamp(subband_prob, min=1e-10))\n            total_rate += rate.mean()\n        \n        return total_rate / len(quantized_subbands)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Color conversion\n\ndef rgb_to_ycbcr(x):\n    \"\"\"Convert RGB to YCbCr\"\"\"\n    matrix = torch.tensor([[0.299, 0.587, 0.114],\n                          [-0.168736, -0.331264, 0.5],\n                          [0.5, -0.418688, -0.081312]]).to(x.device)\n    ycbcr = torch.einsum('ij,bjhw->bihw', matrix, x)\n    ycbcr[:, 1:] += 0.5  # Center Cb, Cr\n    return ycbcr\n\ndef ycbcr_to_rgb(x):\n    \"\"\"Convert YCbCr to RGB\"\"\"\n    matrix = torch.tensor([[1.0, 0.0, 1.402],\n                          [1.0, -0.344136, -0.714136],\n                          [1.0, 1.772, 0.0]]).to(x.device)\n    rgb = x.clone()\n    rgb[:, 1:] -= 0.5  # Uncenter Cb, Cr\n    rgb = torch.einsum('ij,bjhw->bihw', matrix, rgb)\n    return torch.clamp(rgb, 0, 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#IWAVEV3 OBJ\nclass iWaveV3_Obj(iWaveV3_Base):\n    def __init__(self, levels=3, transform_type='affine'):\n        super(iWaveV3_Obj, self).__init__(levels, transform_type)\n        self.dequantization = DequantizationModule()\n        \n    def forward(self, x, training=True):\n        # Forward transform\n        subbands = self.transform(x)\n        \n        # Adaptive quantization\n        if training:\n            quantized_subbands = [self.channel_aware_quantize(sb, i) for i, sb in enumerate(subbands)]\n        else:\n            quantized_subbands = [self.channel_aware_quantize(sb, i) for i, sb in enumerate(subbands)]\n        \n        # Entropy coding\n        entropy_params = self.entropy_model(quantized_subbands)\n        \n        # Inverse transform\n        reconstructed = self.transform.inverse(quantized_subbands)\n        \n        # Dequantization\n        reconstructed = self.dequantization(reconstructed)\n        \n        # Calculate rate with improved method\n        rate = self.calculate_rate_improved(quantized_subbands, entropy_params) if training else 0\n        \n        return reconstructed, quantized_subbands, rate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#IwaveV3 perp\nclass iWaveV3_Perp(iWaveV3_Base):\n    def __init__(self, levels=3, transform_type='affine'):\n        super(iWaveV3_Perp, self).__init__(levels, transform_type)\n        self.dequantization = DequantizationModule()\n        self.perceptual_pp = PerceptualPostProcessing()\n        \n    def forward(self, x, training=True, use_perceptual=True):\n        # Forward transform\n        subbands = self.transform(x)\n        \n        # Adaptive quantization\n        if training:\n            quantized_subbands = [self.channel_aware_quantize(sb, i) for i, sb in enumerate(subbands)]\n        else:\n            quantized_subbands = [self.channel_aware_quantize(sb, i) for i, sb in enumerate(subbands)]\n        \n        # Entropy coding\n        entropy_params = self.entropy_model(quantized_subbands)\n        \n        # Inverse transform\n        reconstructed = self.transform.inverse(quantized_subbands)\n        \n        # Dequantization\n        reconstructed = self.dequantization(reconstructed)\n        \n        # Perceptual post-processing\n        if use_perceptual:\n            reconstructed = self.perceptual_pp(reconstructed)\n        \n        # Calculate rate with improved method\n        rate = self.calculate_rate_improved(quantized_subbands, entropy_params) if training else 0\n        \n        return reconstructed, quantized_subbands, rate\nprint(\"Optimized model components defined!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== OPTIMIZED METRICS & UTILITIES ====================\n\nclass VGGPerceptualLoss(nn.Module):\n    def __init__(self):\n        super(VGGPerceptualLoss, self).__init__()\n        vgg = vgg19(pretrained=True).features\n        self.slice1 = nn.Sequential(*list(vgg)[:2])\n        \n        for param in self.parameters():\n            param.requires_grad = False\n    \n    def forward(self, x, target):\n        x = (x - 0.5) * 2\n        target = (target - 0.5) * 2\n        \n        x_feat = self.slice1(x)\n        target_feat = self.slice1(target)\n        \n        return F.mse_loss(x_feat, target_feat)\n\ndef calculate_metrics(original, reconstructed):\n    # Move tensors to CPU and convert to numpy\n    original_np = original.squeeze().permute(1, 2, 0).cpu().numpy()\n    reconstructed_np = reconstructed.squeeze().permute(1, 2, 0).cpu().numpy()\n    \n    original_np = np.clip(original_np, 0, 1)\n    reconstructed_np = np.clip(reconstructed_np, 0, 1)\n    \n    psnr = peak_signal_noise_ratio(original_np, reconstructed_np, data_range=1.0)\n    \n    min_dim = min(original_np.shape[0], original_np.shape[1])\n    win_size = min(7, min_dim)\n    if win_size % 2 == 0:\n        win_size -= 1\n    \n    try:\n        ssim = structural_similarity(original_np, reconstructed_np, \n                                   win_size=win_size, channel_axis=2, data_range=1.0)\n    except:\n        ssim = 0.5\n    \n    return psnr, ssim\n\ndef calculate_bpp_improved(quantized_subbands, image_size):\n    \"\"\"Improved BPP calculation considering entropy\"\"\"\n    total_bits = 0\n    for subband in quantized_subbands:\n        # Move to CPU for numpy operations\n        subband_cpu = subband.cpu()\n        # Estimate bits using entropy\n        unique_vals = torch.unique(subband_cpu)\n        hist = torch.histc(subband_cpu, bins=len(unique_vals))\n        prob = hist / hist.sum()\n        entropy = -torch.sum(prob * torch.log2(prob + 1e-8))\n        total_bits += entropy * subband_cpu.numel()\n    \n    total_pixels = image_size[0] * image_size[1]\n    return total_bits / total_pixels\n\ndef calculate_bpp_simple(quantized_subbands, image_size):\n    \"\"\"Simple BPP calculation as fallback\"\"\"\n    total_elements = sum(sb.numel() for sb in quantized_subbands)\n    total_pixels = image_size[0] * image_size[1]\n    bpp = (total_elements * 2) / total_pixels\n    return bpp\n\ndef load_and_preprocess_image(image_path, target_size=None):\n    \"\"\"\n    Enhanced image loading with better preprocessing\n    \"\"\"\n    if target_size is None:\n        target_size = (config.image_size, config.image_size)\n    \n    try:\n        image = PILImage.open(image_path).convert('RGB')\n        original_size = image.size  # (width, height)\n        \n        # Convert to tensor and resize\n        transform = transforms.Compose([\n            transforms.Resize(target_size),\n            transforms.ToTensor()\n        ])\n        \n        image_tensor = transform(image)\n        processed_size = (image_tensor.shape[2], image_tensor.shape[1])  # (height, width)\n        \n        return image_tensor.unsqueeze(0), original_size, processed_size\n        \n    except Exception as e:\n        print(f\" Error loading image {image_path}: {e}\")\n        # Return dummy tensor\n        dummy_tensor = torch.zeros(1, 3, target_size[0], target_size[1])\n        return dummy_tensor, target_size, target_size\n\nprint(\" Optimized metrics and utilities defined!\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== CLEAN OUTPUT VISUALIZATION ====================\n\ndef display_clean_results(original_tensor, iwave_obj, iwave_perp, device, image_path):\n    \"\"\"\n    Display clean results: Original, Compressed (both methods), Reconstructed\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"  CLEAN COMPRESSION RESULTS\")\n    print(\"=\"*60)\n    \n    # Set quantization step for testing\n    qstep = 0.025\n    iwave_obj.qstep.data = torch.tensor(qstep).to(device)\n    iwave_perp.qstep.data = torch.tensor(qstep).to(device)\n    \n    with torch.no_grad():\n        # Compress with both methods\n        compressed_obj, quantized_obj, _ = iwave_obj(original_tensor, training=False)\n        compressed_perp, quantized_perp, _ = iwave_perp(original_tensor, training=False, use_perceptual=True)\n        \n        # Reconstruct from compressed data (simulating loading from file)\n        reconstructed_obj, _, _ = iwave_obj(compressed_obj, training=False)\n        reconstructed_perp, _, _ = iwave_perp(compressed_perp, training=False, use_perceptual=True)\n        \n        # Calculate metrics for compressed images\n        psnr_obj, ssim_obj = calculate_metrics(original_tensor, compressed_obj)\n        psnr_perp, ssim_perp = calculate_metrics(original_tensor, compressed_perp)\n        \n        # Calculate metrics for reconstructed images\n        psnr_recon_obj, ssim_recon_obj = calculate_metrics(original_tensor, reconstructed_obj)\n        psnr_recon_perp, ssim_recon_perp = calculate_metrics(original_tensor, reconstructed_perp)\n        # Calculate BPP\n        try:\n            bpp_obj = calculate_bpp_improved(quantized_obj, (original_tensor.shape[2], original_tensor.shape[3]))\n            bpp_perp = calculate_bpp_improved(quantized_perp, (original_tensor.shape[2], original_tensor.shape[3]))\n        except:\n            bpp_obj = calculate_bpp_simple(quantized_obj, (original_tensor.shape[2], original_tensor.shape[3]))\n            bpp_perp = calculate_bpp_simple(quantized_perp, (original_tensor.shape[2], original_tensor.shape[3]))\n\n    # Convert tensors for display\n    original_display = original_tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n    compressed_obj_display = compressed_obj.squeeze().permute(1, 2, 0).cpu().numpy()\n    compressed_perp_display = compressed_perp.squeeze().permute(1, 2, 0).cpu().numpy()\n    recon_obj_display = reconstructed_obj.squeeze().permute(1, 2, 0).cpu().numpy()\n    recon_perp_display = reconstructed_perp.squeeze().permute(1, 2, 0).cpu().numpy()\n\n    # Create clean visualization\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    # Row 1: Original and Compressed Images\n    axes[0, 0].imshow(np.clip(original_display, 0, 1))\n    axes[0, 0].set_title('1. Original Image', fontsize=16, fontweight='bold', pad=20)\n    axes[0, 0].axis('off')\n    \n    axes[0, 1].imshow(np.clip(compressed_obj_display, 0, 1))\n    axes[0, 1].set_title('2. iWaveV3-Obj Compressed', fontsize=16, fontweight='bold', pad=20)\n    axes[0, 1].axis('off')\n    \n    axes[0, 2].imshow(np.clip(compressed_perp_display, 0, 1))\n    axes[0, 2].set_title('3. iWaveV3-Perp Compressed', fontsize=16, fontweight='bold', pad=20)\n    axes[0, 2].axis('off')\n\n        # Row 2: Reconstructed Images\n    axes[1, 0].imshow(np.clip(original_display, 0, 1))\n    axes[1, 0].set_title('Reference (Original)', fontsize=14, fontweight='bold', pad=20)\n    axes[1, 0].axis('off')\n    \n    axes[1, 1].imshow(np.clip(recon_obj_display, 0, 1))\n    axes[1, 1].set_title('4. iWaveV3-Obj Reconstructed', fontsize=16, fontweight='bold', pad=20)\n    axes[1, 1].axis('off')\n    \n    axes[1, 2].imshow(np.clip(recon_perp_display, 0, 1))\n    axes[1, 2].set_title('5. iWaveV3-Perp Reconstructed', fontsize=16, fontweight='bold', pad=20)\n    axes[1, 2].axis('off')\n    \n    # Add text annotations with metrics\n    fig.text(0.15, 0.48, f'PSNR: {psnr_obj:.2f} dB\\nSSIM: {ssim_obj:.4f}\\nBPP: {bpp_obj:.3f}', \n             fontsize=12, ha='center', va='center', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n    fig.text(0.5, 0.48, f'PSNR: {psnr_perp:.2f} dB\\nSSIM: {ssim_perp:.4f}\\nBPP: {bpp_perp:.3f}', \n             fontsize=12, ha='center', va='center', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n    fig.text(0.15, 0.02, f'PSNR: {psnr_recon_obj:.2f} dB\\nSSIM: {ssim_recon_obj:.4f}', \n             fontsize=12, ha='center', va='center', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n    fig.text(0.5, 0.02, f'PSNR: {psnr_recon_perp:.2f} dB\\nSSIM: {ssim_recon_perp:.4f}', \n             fontsize=12, ha='center', va='center', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n    \n    plt.tight_layout()\n    plt.subplots_adjust(bottom=0.1, top=0.9)\n    plt.savefig(f\"{config.model_dir}clean_compression_results.png\", dpi=300, bbox_inches='tight')\n    plt.show()\n    # Print detailed metrics table\n    print(\"\\n\" + \"=\"*80)\n    print(\" DETAILED COMPRESSION METRICS\")\n    print(\"=\"*80)\n    print(f\"{'Stage':<25} {'Model':<15} {'PSNR(dB)':<10} {'SSIM':<8} {'BPP':<8}\")\n    print(\"-\"*80)\n    print(f\"{'Compressed':<25} {'iWaveV3-Obj':<15} {psnr_obj:<10.2f} {ssim_obj:<8.4f} {bpp_obj:<8.3f}\")\n    print(f\"{'Compressed':<25} {'iWaveV3-Perp':<15} {psnr_perp:<10.2f} {ssim_perp:<8.4f} {bpp_perp:<8.3f}\")\n    print(f\"{'Reconstructed':<25} {'iWaveV3-Obj':<15} {psnr_recon_obj:<10.2f} {ssim_recon_obj:<8.4f} {'-':<8}\")\n    print(f\"{'Reconstructed':<25} {'iWaveV3-Perp':<15} {psnr_recon_perp:<10.2f} {ssim_recon_perp:<8.4f} {'-':<8}\")\n    print(\"-\"*80)\n    \n    # Save individual images\n    save_individual_images(original_tensor, compressed_obj, compressed_perp, \n                          reconstructed_obj, reconstructed_perp)\n    \n    return {\n        'compressed_metrics': {\n            'obj_psnr': psnr_obj, 'obj_ssim': ssim_obj, 'obj_bpp': bpp_obj,\n            'perp_psnr': psnr_perp, 'perp_ssim': ssim_perp, 'perp_bpp': bpp_perp\n        },\n        'reconstructed_metrics': {\n            'obj_psnr': psnr_recon_obj, 'obj_ssim': ssim_recon_obj,\n            'perp_psnr': psnr_recon_perp, 'perp_ssim': ssim_recon_perp\n        }\n    }\n\n\n    \n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Saving individual images\ndef save_individual_images(original, compressed_obj, compressed_perp, recon_obj, recon_perp):\n    \"\"\"Save all individual images\"\"\"\n    # Convert to PIL and save\n    original_pil = transforms.ToPILImage()(original.squeeze().cpu())\n    compressed_obj_pil = transforms.ToPILImage()(compressed_obj.squeeze().cpu())\n    compressed_perp_pil = transforms.ToPILImage()(compressed_perp.squeeze().cpu())\n    recon_obj_pil = transforms.ToPILImage()(recon_obj.squeeze().cpu())\n    recon_perp_pil = transforms.ToPILImage()(recon_perp.squeeze().cpu())\n    \n    original_pil.save(f\"{config.model_dir}1_original.png\")\n    compressed_obj_pil.save(f\"{config.model_dir}2_compressed_obj.png\")\n    compressed_perp_pil.save(f\"{config.model_dir}3_compressed_perp.png\")\n    recon_obj_pil.save(f\"{config.model_dir}4_reconstructed_obj.png\")\n    recon_perp_pil.save(f\"{config.model_dir}5_reconstructed_perp.png\")\n    \n    print(f\"\\n Individual images saved:\")\n    print(f\"   1. {config.model_dir}1_original.png\")\n    print(f\"   2. {config.model_dir}2_compressed_obj.png\")\n    print(f\"   3. {config.model_dir}3_compressed_perp.png\")\n    print(f\"   4. {config.model_dir}4_reconstructed_obj.png\")\n    print(f\"   5. {config.model_dir}5_reconstructed_perp.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== OPTIMIZED TRAINING ====================\n\ndef initialize_models(device):\n    iwave_obj = iWaveV3_Obj(levels=config.levels, transform_type='affine').to(device)\n    iwave_perp = iWaveV3_Perp(levels=config.levels, transform_type='affine').to(device)\n    \n    def init_weights(m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.xavier_uniform_(m.weight, gain=0.5)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n    \n    iwave_obj.apply(init_weights)\n    iwave_perp.apply(init_weights)\n    \n    return iwave_obj, iwave_perp\ndef progressive_training(model, dataloader, device, model_name):\n    \"\"\"Progressive training for better compression\"\"\"\n    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=1e-5)\n    mse_loss = nn.MSELoss()\n    perceptual_loss = VGGPerceptualLoss().to(device) if \"Perp\" in model_name else None\n    \n    model.train()\n    \n    for epoch in range(config.num_epochs):\n        # Progressive quantization - start easy, get harder\n        if epoch < 15:\n            model.qstep.data = torch.tensor(0.02)  # Easy\n        elif epoch < 35:\n            model.qstep.data = torch.tensor(0.03)  # Medium\n        else:\n            model.qstep.data = torch.tensor(0.04)  # Hard (more compression)\n        \n        total_loss = 0\n        num_batches = 0\n        \n        for batch_imgs, paths in dataloader:\n            batch_imgs = batch_imgs.to(device)\n            optimizer.zero_grad()\n            \n            if \"Perp\" in model_name:\n                reconstructed, _, rate = model(batch_imgs, training=True, use_perceptual=True)\n                distortion = mse_loss(reconstructed, batch_imgs)\n                percep = perceptual_loss(reconstructed, batch_imgs)\n                loss = distortion + 0.01 * percep + config.rate_weight * rate\n                \n            else:\n                reconstructed, _, rate = model(batch_imgs, training=True)\n                distortion = mse_loss(reconstructed, batch_imgs)\n                loss = distortion + config.rate_weight * rate\n            \n            if not torch.isnan(loss) and not torch.isinf(loss):\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n                optimizer.step()\n                \n                total_loss += loss.item()\n                num_batches += 1\n        \n        if num_batches > 0 and (epoch + 1) % 10 == 0:\n            avg_loss = total_loss / num_batches\n            current_qstep = model.qstep.item()\n            print(f'   Epoch {epoch+1}/{config.num_epochs}, Loss: {avg_loss:.6f}, Qstep: {current_qstep:.4f}')\n    \n    model.eval()\n    return model\n\ndef train_models_on_dataset():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\" Using device: {device}\")\n    \n    image_paths = glob.glob(config.train_data_path)\n    print(f\" Found {len(image_paths)} images for training\")\n    \n    dataset = ImageCompressionDataset(image_paths, image_size=config.image_size)\n    dataloader = DataLoader(\n        dataset, \n        batch_size=config.batch_size, \n        shuffle=True, \n        num_workers=0,\n        collate_fn=custom_collate_fn\n    )\n    \n    iwave_obj, iwave_perp = initialize_models(device)\n    \n    print(f\" Starting progressive training for {config.num_epochs} epochs...\")\n    \n    print(\"\\n Training iWaveV3-Obj with progressive compression...\")\n    iwave_obj = progressive_training(iwave_obj, dataloader, device, \"iWaveV3-Obj\")\n    \n    print(\"\\n Training iWaveV3-Perp with progressive compression...\")\n    iwave_perp = progressive_training(iwave_perp, dataloader, device, \"iWaveV3-Perp\")\n    \n    print(\" Training completed!\")\n    return iwave_obj, iwave_perp, device","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== TESTING INTERFACE ====================\n\ndef test_single_image_clean(iwave_obj, iwave_perp, device):\n    \"\"\"\n    Clean testing function that shows exactly what you need\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\" CLEAN TESTING INTERFACE\")\n    print(\"=\"*60)\n    \n    test_image_path = input(\" Enter the path to your test image: \").strip()\n    \n    if not os.path.exists(test_image_path):\n        print(\" Image path not found! Using default image...\")\n        image_paths = glob.glob(config.train_data_path)\n        test_image_path = image_paths[0] if image_paths else \"/kaggle/input/archive/kodim03.png\"\n    \n    print(f\" Processing: {test_image_path}\")\n    \n    image_tensor, original_size, processed_size = load_and_preprocess_image(test_image_path)\n    image_tensor = image_tensor.to(device)\n    \n    print(f\" Original size: {original_size}, Processed size: {processed_size}\")\n\n# Test different compression levels\n    compression_levels = [\n        {'name': 'High Quality', 'qstep': 0.015},\n        {'name': 'Balanced', 'qstep': 0.025},\n        {'name': 'High Compression', 'qstep': 0.035},\n    ]\n    \n    print(f\"\\n Testing {len(compression_levels)} compression levels...\")\n    \n    for level in compression_levels:\n        print(f\"\\n {level['name']} (qstep={level['qstep']}):\")\n        \n        iwave_obj.qstep.data = torch.tensor(level['qstep']).to(device)\n        iwave_perp.qstep.data = torch.tensor(level['qstep']).to(device)\n        \n        with torch.no_grad():\n            # Compress\n            compressed_obj, quantized_obj, _ = iwave_obj(image_tensor, training=False)\n            compressed_perp, quantized_perp, _ = iwave_perp(image_tensor, training=False, use_perceptual=True)\n            \n            # Calculate metrics\n            psnr_obj, ssim_obj = calculate_metrics(image_tensor, compressed_obj)\n            psnr_perp, ssim_perp = calculate_metrics(image_tensor, compressed_perp)\n\n            try:\n                bpp_obj = calculate_bpp_improved(quantized_obj, processed_size)\n                bpp_perp = calculate_bpp_improved(quantized_perp, processed_size)\n            except:\n                bpp_obj = calculate_bpp_simple(quantized_obj, processed_size)\n                bpp_perp = calculate_bpp_simple(quantized_perp, processed_size)\n            \n            print(f\"   iWaveV3-Obj:  PSNR: {psnr_obj:.2f}dB, SSIM: {ssim_obj:.4f}, BPP: {bpp_obj:.3f}\")\n            print(f\"   iWaveV3-Perp: PSNR: {psnr_perp:.2f}dB, SSIM: {ssim_perp:.4f}, BPP: {bpp_perp:.3f}\")\n    \n    # Display clean results with the balanced setting\n    print(f\"\\nðŸŽ¯ Displaying detailed results with Balanced compression...\")\n    iwave_obj.qstep.data = torch.tensor(0.025).to(device)\n    iwave_perp.qstep.data = torch.tensor(0.025).to(device)\n    \n    results = display_clean_results(image_tensor, iwave_obj, iwave_perp, device, test_image_path)\n    \n    return results\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== MAIN EXECUTION ====================\n\nif __name__ == \"__main__\":\n    print(\" iWaveV3 - Clean Compression Pipeline\")\n    print(\"=\" * 60)\n    \n    start_time = time.time()\n    \n    try:\n        # Train models\n        iwave_obj, iwave_perp, device = train_models_on_dataset()\n        \n        # Test with clean interface\n        test_results = test_single_image_clean(iwave_obj, iwave_perp, device)\n        \n        total_time = time.time() - start_time\n        print(f\"\\n  Total execution time: {total_time:.2f} seconds\")\n        print(\" Clean compression pipeline completed successfully!\")\n        \n        print(f\"\\n All results saved in: {config.model_dir}\")\n        print(\"   - clean_compression_results.png (Complete visualization)\")\n        print(\"   - 1_original.png to 5_reconstructed_perp.png (Individual images)\")\n        \n    except Exception as e:\n        print(f\" Error during execution: {e}\")\n        import traceback\n        traceback.print_exc()\n\nprint(\" iWaveV3 Clean Output Pipeline Ready!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}